## 3D Scene Perception, Embeddings and Neural Rendering
*3D Scene Perception, Embeddings and Neural Rendering* is a [WASP](https://wasp-sweden.org/)-funded NEST (Novelty, Excellence, Synergy, Teams) project, a collaborative project that involves research groups from Chalmers University of Technology, Lund University, and KTH Royal Institute of Technology.

## Work packages
 
#### WP1.1: Latent embeddings and disentanglement. Kathlén Kohn (lead), Mårten Björkman (colead)

Disentanglement is a widely used concept and one of the most ambitious challenges in learning. Yet, it still lacks a widely accepted formal definition. The challenge is to develop learning algorithms that disentangle the different factors of variation in the data, but what exactly that means highly depends on the application at hand. For instance, in order to be able to modify content of an indoor scene, we seek algorithms to learn situated and semantic encodings (e.g., positions and colors of objects or lighting). Several mathematical ideas have been proposed to provide a formal definition of disentanglement (e.g., statistical independence, [flattening manifolds](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444/#:~:text=Mounting%20evidence%20suggests%20that%20%E2%80%9Ccore,in%20the%20inferior%20temporal%20cortex.), [irreducible representations of groups](https://arxiv.org/abs/1402.4437), [direct products of group actions](https://arxiv.org/abs/1812.02230), but none of them applies to all practical settings where disentanglement is used. We aim to develop a more generally applicable formal definition and to study theoretical guarantees of what can be achieved with disentanglement.

Autoencoders are at the core of our project. An important aspect is their ability to memorize the training data, which has been [recently explored](https://arxiv.org/abs/1909.12362) from a dynamical systems perspective. Empirical results suggest that training examples form attractors of these autoencoders, but the theoretical reasons behind that mechanism are still not clear. Algebraic techniques can be applied in the setting of ReLU autoencoders with Euclidean loss, as the underlying geometric problem is to find a closest point on a semi-algebraic set. An open conjecture is that all training examples are attractors in a (global) minimum of the Euclidean loss of a sufficiently deep ReLU autoencoder. We aim to investigate that conjecture as well as further conditions under which attractors are formed.

#### WP1.2: Generalization and invariance. Cristian Sminchisescu (lead), Fredrik Kahl (colead)

One of the key factors for the success of deep learning for 2D image interpretation is the convolutional layer, which induces translation equivariance. That is, if the input is translated, then the output is too. It drastically reduces the number of learnable parameters, compared to a fully connected layer, and increases generalization performance. For 3D data, like point clouds, meshes and graphs, other equivariances have been investigated for classification, but except for convolutions, they are not exploited in encoder-decoder models. We will analyze, construct and develop efficient implementations for other equivariances, both for
the encoder *h<sub>ϕ</sub> ◦ g(X) = g ◦ h<sub>ϕ</sub>(X)* and similarly for the decoder, *f<sub>θ</sub> ◦ g(Z) = g ◦ f<sub>θ</sub>(Z)* for all *g* belonging to a group. We will investigate the groups of *SO(3)*, *SE(3)* and their subgroups, for instance, azimuthal rotations. <!-- Another benefit is that the pose of the 3D representation is naturally separated (disentangled), and hence, directly controllable via the embedding *Z*. In a similar strand, as the embeddings encode the different scene objects, and there is no natural order among them, the 3D representation should be permutation equivariant.-->

[Current encoder-decoder models](https://arxiv.org/abs/1911.06971) perform well when trained on individual categories, (e.g., “cars”), but are not able to simultaneously cope with multiple categories (e.g., “cars”, “trucks” and “buses”). By combining invariance with other generalization techniques such as [multi-task learning](https://arxiv.org/abs/2010.08244v1), 3D data augmentation and self-supervision (see WP2.2), we expect to bridge the gap in building flexible generative models.

#### WP2.1: Dynamics and deformation. Mårten Björkman (lead), Cristian Sminchisescu (colead)
#### WP2.2: Flexible and controllable generative models. Fredrik Kahl (lead), Kathlén Kohn (colead)


